# mysql面试题

## 1. 通用问题



### 1. 数据库的三大范式



1. 第一范式：每个列都不可以再拆分。 
2. 第二范式：在第一范式的基础上，非主键列完全依赖于主键，而不能是依赖于主键的一部分。 
3. 第三范式：在第二范式的基础上，非主键列只依赖于主键，不依赖于其他非主键。



在设计数据库结构的时候，要尽量遵守三范式，如果不遵守，必须有足够的理由。比如性能。事实上我们经常会为了性能而妥协数据库的设计。



### 2. MySQL的基本架构

![img](https://cdn.nlark.com/yuque/0/2021/png/22228669/1635921406671-6c1d5dec-1414-45ed-b2c4-2b001c42d9c7.png?x-oss-process=image%2Fresize%2Cw_683%2Climit_0)

大体来说，MySQL可以分为**Server层和存储引擎层**两部分。

Server层包括连接器、查询缓存、分析器、优化器、执行器等，涵盖MySQL的大多数核心服务功能，以及所有的内置函数（如日期、时间、数学和加密函数等），所有跨存储引擎的功能都在这一层实现，比如存储过程、触发器、视图等。

而存储引擎层负责数据的存储和提取。其架构模式是插件式的，支持InnoDB、MyISAM、Memory等多个存储引擎。现在最常用的是InnoDB，它从MySQL5.5.5版本开始成为默认储存引擎。

![img](https://cdn.nlark.com/yuque/0/2022/jpeg/22228669/1645450477321-a330c418-3bf2-4c54-a1b4-92d7673cf544.jpeg)



### 3. MyISAM引擎的特点



1.  不支持事务操作，ACID 的特性也就不存在了，这一设计是为了性能和效率考虑的。
2.  不支持外键操作，如果强行增加外键，MySQL 不会报错，只不过外键不起作用。 
3.  MyISAM 默认的锁粒度是表级锁，所以并发性能比较差，加锁比较快，锁冲突比较少，不太容易发生死的情况。 
4. MyISAM 会在磁盘上存储三个文件，文件名和表名相同，扩展名分别是frm(存储表定义)、MYD(MYData，存储数据)、MYI(MyIndex，存储索引)。这里需要特别注意的是 MyISAM 只缓存索引文件，并不缓存数据文件。 
5. MyISAM 支持的索引类型有全局索引(Full-Text)、B-Tree 索引、R-Tree 索引。

1. 1. Full-Text 索引:它的出现是为了解决针对文本的模糊查询效率较低的问题。 
   2. B-Tree 索引: 所有的索引节点都按照平衡树的数据结构来存储，所有的索引数据节点都在叶节点 
   3. R-Tree 索引:它的存储方式和 B-Tree 索引有一些区别，主要设计用于存储空间和多维数据的字段做索                           引目前的 MySQL 版本仅支持 geometry类型的字段作索引，相对于 BTREE,RTREE 的优                             势在于范围查找。 

1. 数据库所在主机如果宕机，MyISAM 的数据文件容易损坏，而且难以恢复。 
2. 增删改查性能方面:SELECT 性能较高，适用于查询较多的情况

### 4.  INNOD引擎的特点 背！



1. 支持事务操作，具有事务 ACID 隔离特性，默认的隔离级别是可重复读(repetable-read)、通过 MVCC(并发版本控制)来实现的。能够解决脏读和不可重复读的问题。 
2. InnoDB 支持外键操作。 
3. InnoDB 默认的锁粒度行级锁，并发性能比较好，会发生死锁的情况。 
4. 和 MyISAM 一样的是，InnoDB 存储引擎也有 frm 文件存储表结构定义，但是不同的是，InnoDB 的表数据与索引数据是存储在一起的，都位于 B+数的叶子节点上，而 MylSAM 的表数据和索引数据是分开的。 
5. InnoDB 有安全的日志文件，这个日志文件用于恢复因数据库崩溃或其他情况导致的数据丢失问题，保证数据的一致性。 
6. InnoDB 和 MylSAM 支持的索引类型相同，但具体实现因为文件结构的不同有很大差异。 
7. 增删改查性能方面，果执行大量的增删改操作，推荐使用 InnoDB 存储引擎，它在删除操作时是对行删除，不会重建表。



1. 

## 

### 5.  SQL优化的方法 背！



1. 查询语句无论是使用哪种判断条件等于、小于、大于，WHERE 左侧的条件查询字段不要使用函数或者表达式。
2. 使用 EXPLAIN 命令优化你的 SELECT 查询，对于复杂、效率低的 SQL 语句，我们通常是使用 explainsql 来分析这条 SQL 语句，这样方便我们分析，进行优化。 
3. 当你的 SELECT 查询语句只需要使用一条记录时，要使用 LIMIT 1。不要直接使用 SELECT*，而应该使用具体需要查询的表字段，因为使用 EXPLAIN进行分析时，SELECT"使用的是全表扫描，也就是 type =all 。 
4. 为每一张表设置一个 ID 属性。 
5. 避免在 MHERE 字句中对字段进行 NULL 。
6. 判断避免在 WHERE 中使用!或>操作符 。
7. 使用 BETWEEN AND 替代 IN 。
8. 为搜索字段创建索引 。
9. 选择正确的存储引擎，InnoDB、MyISAM、MEMORY 等。
10. 使用 LIKE%abc%不会走索引，而使用 LIKE abc%会走索引。 
11. 对于枚举类型的字段(即有固定罗列值的字段)，建议使用 ENUM 而不是VARCHAR，如性别、星期、类型、类别等。 
12. 拆分大的 DELETE 或 INSERT 语句 。
13. 选择合适的字段类型，选择标准是尽可能小、尽可能定长、尽可能使用整数。 
14. 字段设计尽可能使用 NOT NULL 。
15. 进行水平切割或者垂直分割。
16. 合理的使用join。
17. 最重要的是学会使用慢查询日志与explain。

### 6.  误删数据如何恢复？



**使用 delete 语句误删数据行。**



可以用 Flashback 工具通过闪回把数据恢复回来。

Flashback 恢复数据的原理，是修改 binlog 的内容，拿回原库重放。而能够使用这个方案的前提是，需要确保 binlog_format=row 和 binlog_row_image=FULL。



**使用 drop table 或者 truncate table 语句误删数据表；**



这种情况下，要想恢复数据，就需要使用全量备份，加增量日志的方式了。这个方案要求线上有定期的全量备份，并且实时备份 binlog。



**使用 rm 命令误删整个 MySQL 实例。**



其实，对于一个有高可用机制的 MySQL 集群来说，最不怕的就是 rm 删除数据了。只要不是恶意地把整个集群删除，而只是删掉了其中某一个节点的数据的话，HA 系统就会开始工作，选出一个新的主库，从而保证整个集群的正常工作。

这时，你要做的就是在这个节点上把数据恢复回来，再接入整个集群。

### 7.  自增主键不连续？



**唯一键冲突是导致自增主键 id 不连续的第一种原因。**



**事务回滚是导致自增主键 id 不连续的第二种原因。**



**批量申请自增 id是导致自增主键id不连续的第三种原因。**



实际上，**表的结构定义存放在后缀名为.frm 的文件中，但是并不会保存自增值。**



在 MySQL 5.7 及之前的版本，自增值保存在内存里，并没有持久化。



在 MySQL 8.0 版本，将自增值的变更记录在了 redo log 中，重启的时候依靠 redo log 恢复重启之前的值。

### 8.  自增id用完了怎么办？



4294967295，这个数字已经可以应付大部分的场景了，如果你的服务会经常性的插入和删除数据的话，还是存在用完的风险，**建议采用bigint unsigned**，这个数字就大了。



**不过，还存在另一种情况，如果在创建表没有显示申明主键，会怎么办？**



如果是这种情况，InnoDB会自动帮你创建一个不可见的、长度为6字节的row_id，而且InnoDB 维护了一个全局的 dictsys.row_id，所以未定义主键的表都共享该row_id，每次插入一条数据，都把全局row_id当成主键id，然后全局row_id加1



该全局row_id在代码实现上使用的是bigint unsigned类型，但实际上只给row_id留了6字节，这种设计就会存在一个问题：如果全局row_id一直涨，一直涨，直到2的48幂次-1时，这个时候再+1，row_id的低48位都为0，结果在插入新一行数据时，拿到的row_id就为0，存在主键冲突的可能性。

### 9.  count(1),count(*)效率？



按照效率排序的话，count(字段)<count(主键 id)<count(1)≈count(*)，所以建议，尽量使用 count(*)。



### 10. 表的数据信息存在哪里？背！



表数据信息可能较⼩也可能巨⼤⽆⽐，可以存储在共享表空间⾥，也可以单独存储在⼀个以.ibd为后缀的⽂件⾥， 由参数 innodb_file_per_table 来控制，建议总是作为⼀个单独的⽂件来存储，这样⾮常容易管理，并且在不需要的时候，使⽤ drop table 命令也能直接把对应的⽂件删除，如果存储在共享空间之中即使表删除了空间也不会释放。



### 11.表的结构信息存在哪⾥？背！



⾸先，表结构定义占有的存储空间⽐较⼩。

MySQL8.0 之前：表结构的定义信息存在以.frm为后缀的⽂件⾥ 

MySQL8.0 之后：则允许把表结构的定义信息存在系统数据表之中

系统数据表，主要⽤于存储MySQL的系统数据，⽐如：数据字典、undo log(默认)等⽂件



### 12.空洞是啥？咋产⽣的？



空洞就是那些被**标记可复⽤但是还没被使⽤的存储空间。**

使⽤delete命令删除数据会产⽣空洞，标记为可复⽤。

插⼊新的数据可能引起⻚分裂，也可能产⽣空洞。

修改操作，有时是⼀种先删后插的动作也可能产⽣空洞。



### 13. order by 的工作流程 

```go
select city,name,age from t where city='杭州' order by name limit 1000; 
```

涉及到⽤户语句的排序，mysql 会给每个线程分配⼀块内存⽤于排序，也就是 sort_buffer。 

这条语句的执⾏逻辑是：



**1、先初始化 sort_bufer** 

**2****、然后放⼊** **city,name,age** **字段**，不断地由主键id索引到整⾏再到三个字段的值，匹配查找的值存⼊ sort_buff 

**3****、然后按** **name** **排序，返回前** **1000** **个值** 



但是如果 sort_buffer_size 设置的太⼩，⽆法存放所有匹配的字段，排序就⽆法在内存中完成需要借鉴磁盘临时⽂件辅助排序，可以通过 number_of_tmp_files 这个标识来判断是否使⽤，其实这个原理和对超⼤数据的排序相同。 



如果要记录的字段太⻓，这样内存⾥能够同时放下的⾏数很少，要分成很多个临时⽂件，排序的性能会很差。这时会换⼀个算法，叫做rowid排序，顾名思义，就是对主键 id 以及排序字段进⾏存放，这样就节省了空间，但是最后需要通过主键 id 去找到之前未取出的字段。对⽐全字段排序，rowid 排序多访问了⼀次表 t 的主键索引。 



**结论：** 

如果 MySQL 实在是担⼼排序内存太⼩，会影响排序效率，才会采⽤ rowid 排序算法，这样排序过程中⼀次可以排序更多⾏，但是需要再回到原表去取数据。如果 MySQL 认为内存⾜够⼤，会优先选择全字段排序，把需要的字段都放到 sort_buffer 中，这样排序后就会直接从内存⾥⾯返回查询结果了，不⽤再回到原表去取数据。 



如果想要避免排序，可以建⽴对应字段的索引。 如果想要进⼀步避免主键 id 的回表查询，可以使⽤覆盖索引，这种情况的索引建⽴成本会⽐较⼤，需要你去⾃⼰权衡是否使⽤.



### 14. 为什么临时表可以重名？



在实际应⽤中，临时表⼀般⽤于处理⽐较复杂的计算逻辑。 



由于临时表是每个线程⾃⼰可⻅的，所以不需要考虑多个线程执⾏同⼀个处理逻辑时，临时表的重名问题。在线程 退出的时候，临时表也能⾃动删除，省去了收尾和异常处理的⼯作。 



在 binlog_format='row’的时候，临时表的操作不记录到 binlog 中，也省去了不少麻烦，防⽌出现主备不⼀致的

这⾥说到的临时表是⽤户临时表，⽽不是内存临时表.



### 15. 什么情况下会产生内部临时表 背！



1. 使用 UNION 查询：UNION 有两种，一种是UNION ，一种是 UNION ALL ，它们都用于联合查询；区别是 使用 UNION 会去掉两个表中的重复数据，相当于对结果集做了一下去重(distinct)。使用 UNION ALL，则不会排重，返回所有的行。使用 UNION 查询会产生临时表，而使用UNION ALL不会产生临时表。
2. 使用 TEMPTABLE 算法或者是 UNION 查询中的视图。TEMPTABLE 算法是一种创建临时表的算法，它是将结果放置到临时表中，意味这要 MySQL 要先创建好一个临时表，然后将结果放到临3时表中去，然后再使用这个临时表进行相应的查询。
3. ORDER BY 和 GROUP BY 的子句不一样时也会产生临时表。
4. DISTINCT 查询并且加上 ORDER BY 时；
5. SQL 用到 SQL_SMALL_RESULT 选项时；如果查询结果比较小的时候，可以加上 SQL_SMALL_RESULT 来优化，产生临时表.
6. FROM 中的子查询；
7. EXPLAIN 查看执行计划结果的 Extra 列中，如果使用 Using Temporary 就表示会用到临时表。



### 16. 要不要使用join?



1. 如果可以使用被驱动表的索引，join 语句还是有其优势的；
2. 不能使用被驱动表的索引，只能使用 Block Nested-Loop Join 算法，这样的语句就尽量不要使用；
3. 在使用 join 的时候，应该让小表做驱动表。更准确地说，在决定哪个表做驱动表的时候，应该是两个表按照各自的条件过滤，过滤完成之后，计算参与 join 的各个字段的总数据量，数据量小的那个表，就是“小表”，应该作为驱动表。



### 17. group-by使⽤原则 背！



1. 如果对 group by 语句的结果没有排序要求，要在语句后⾯加 order by null； 
2. 尽量让 group by 过程⽤上表的索引，确认⽅法是 explain 结果⾥没有 Using temporary 和 Using filesort； 
3. 如果 group by 需要统计的数据量不⼤，尽量只使⽤内存临时表；也可以通过适当调⼤tmp_table_size 参数，来避免⽤到磁盘临时表； 
4. 如果数据量实在太⼤，使⽤ SQL_BIG_RESULT 这个提示，来告诉优化器直接使⽤排序算法得到 group by 的结果，不要使⽤内存临时表.



1. **char 和 varchar 的区别？**



char 表示定长，长度固定，varchar表示变长，即长度可变。char如果插入的长度小于定义长度时，则用空格填充；varchar小于定义长度时，还是按实际长度存储，插入多长就存多长。

因为其长度固定，char的存取速度还是要比varchar要快得多，方便程序的存储与查找；但是char也为此付出的是空间的代价，因为其长度固定，所以会占据多余的空间，可谓是以空间换取时间效率。varchar则刚好相反，以时间换空间。


对 char 来说，最多能存放的字符个数 255，和编码无关。
而 varchar 呢，最多能存放 65532 个字符。varchar的最大有效长度由最大行大小和使用的字符集确定。整体最大长度是 65,532字节。



## 2. 日志



### 1. Mysql有哪些日志，分别有什么作用？



**1：重做日志（redo log）**



确保事务的持久性。redo日志记录事务执行后的状态，用来恢复未写入data file的已成功事务更新的数据。防止在发生故障的时间点，尚有脏页未写入磁盘，在重启[mysql](https://www.2cto.com/database/MySQL/)服务的时候，根据redo log进行重做，从而达到事务的持久性这一特性。



物理格式的日志，记录的是物理数据页面的修改的信息，其redo log是顺序写入redo log file的物理文件中去的。



**2：回滚日志（undo log）**



保证数据的原子性，保存了事务发生之前的数据的一个版本，可以用于回滚，同时可以提供多版本并发控制下的读（MVCC），也即非锁定读。



逻辑格式的日志，在执行undo的时候，仅仅是将数据从逻辑上恢复至事务之前的状态，而不是从物理页面上操作实现的，这一点是不同于redo log的。



**3：归档日志（binlog）**



用于复制，在主从复制中，从库利用主库上的binlog进行重播，实现主从同步。

用于数据库的基于时间点的还原。



**4：错误日志（errorlog）**



错误日志记录着mysqld启动和停止,以及服务器在运行过程中发生的错误的相关信息。在默认情况下，系统记录错误日志的功能是关闭的，错误信息被输出到标准错误输出。



**5：****慢查询日志（slow query log）**



慢日志记录执行时间过长和没有使用索引的查询语句，报错select、update、delete以及insert语句，慢日志只会记录执行成功的语句。



**6：一般查询日志（general log）**



记录了服务器接收到的每一个查询或是命令，无论这些查询或是命令是否正确甚至是否包含语法错误，general log 都会将其记录下来 ，记录的格式为 {Time ，Id ，Command，Argument }。也正因为mysql服务器需要不断地记录日志，开启General log会产生不小的系统开销。 因此，Mysql默认是把General log关闭的。



**7：中继日志（relay log）**



relay-log中继日志是连接master和slave的核心，用于完成主从同步。



### 2.  binlog的三种格式？



1. **statement**



每一条会修改数据的sql都会记录在binlog中。



优点：不需要记录每一行的变化，减少了binlog日志量，节约了IO，提高性能。



缺点：由于记录的只是执行语句，为了这些语句能在slave上正确运行，因此还必须记录每条语句在执行的时候的一些相关信息，以保证所有语句能在slave得到和在master端执行时候相同的结果。另外mysql 的复制,像一些特定函数功能，slave可与master上要保持一致会有很多相关问题(如sleep()函数， last_insert_id()，以及user-defined functions(udf)会出现问题).



1. **row**



不记录sql语句上下文相关信息，仅保存哪条记录被修改。



优点： binlog中可以不记录执行的sql语句的上下文相关的信息，仅需要记录那一条记录被修改成什么了。所以rowlevel的日志内容会非常清楚的记录下每一行数据修改的细节。而且不会出现某些特定情况下的存储过程，或function，以及trigger的调用和触发无法被正确复制的问题



缺点:所有的执行的语句当记录到日志中的时候，都将以每行记录的修改来记录，这样可能会产生大量的日志内容,比如一条update语句，修改多条记录，则binlog中每一条修改都会有记录，这样造成binlog日志量会很大，特别是当执行alter table之类的语句的时候，由于表结构修改，每条记录都发生改变，那么该表每一条记录都会记录到日志中。



1. **mixed**



是以上两种level的混合使用，一般的语句修改使用statment格式保存binlog，如一些函数，statement无法完成主从复制的操作，则采用row格式保存binlog,MySQL会根据执行的每一条具体的sql语句来区分对待记录的日志形式，也就是在Statement和Row之间选择一种.新版本的MySQL中队row level模式也被做了优化，并不是所有的修改都会以row level来记录，像遇到表结构变更的时候就会以statement模式来记录。至于update或者delete等修改数据的语句，还是会记录所有行的变更。



**一般线上的MySQL的binlog日志格式都是 ROW 格式。**



### 3. 重做日志与归档日志的不同？



1. redo log 是 InnoDB 引擎特有的；binlog 是 MySQL 的 Server 层实现的，所有引擎都可以使用。
2. redo log 是物理日志，记录的是“在某个数据页上做了什么修改”；binlog 是逻辑日志，记录的是这个语句的原始逻辑，比如“给 ID=2 这一行的 c 字段加 1 ”。
3. redo log 是循环写的，空间固定会用完；binlog 是可以追加写入的。“追加写”是指 binlog 文件写到一定大小后会切换到下一个，并不会覆盖以前的日志。



### 4. 什么是两阶段提交？ 背！分布式事物！



![img](https://cdn.nlark.com/yuque/0/2022/png/22228669/1641907029577-8ef2e312-0e64-4ea5-b842-636d4580c8af.png?x-oss-process=image%2Fresize%2Cw_750%2Climit_0)

将 redo log 的写入拆成了两个步骤：prepare 和 commit，这就是"两阶段提交"。



通过两阶段提交可以保证两个日志在逻辑上达到一致性的结果。



### 5. 脏页与控制策略？



**当内存数据页跟磁盘数据页内容不一致的时候，我们称这个内存页为“脏页”。**内存数据写入到磁盘后，内存和磁盘上的数据页的内容就一致了，称为“干净页”。



不论是脏页还是干净页，都在内存中。



**刷脏页有可能为以下四种场景：**



**1. InnoDB 的 redo log 写满了。**这时候系统会停止所有更新操作，把 checkpoint 往前推进，redo log 留出空间可以继续写。我在第二讲画了一个 redo log 的示意图，这里我改成环形，便于大家理解。

![img](https://cdn.nlark.com/yuque/0/2022/jpeg/22228669/1642084108901-165a47a1-41d9-41df-9cbb-2287f39faa2d.jpeg)

checkpoint 可不是随便往前修改一下位置就可以的。比如图 2 中，把 checkpoint 位置从 CP 推进到 CP’，就需要将两个点之间的日志（浅绿色部分），对应的所有脏页都 flush 到磁盘上。之后，图中从 write pos 到 CP’之间就是可以再写入的 redo log 的区域。



- 🔲这种情况是 InnoDB 要尽量避免的。因为出现这种情况的时候，整个系统就不能再接受更新了，所有的更新都必须堵住。如果你从监控上看，这时候更新数会跌为 0。



**2. 系统内存不足。**当需要新的内存页，而内存不够用的时候，就要淘汰一些数据页，空出内存给别的数据页使用。如果淘汰的是“脏页”，就要先将脏页写到磁盘。



所以，刷脏页虽然是常态，但是出现以下这两种情况，都是会明显影响性能的：

1. 一个查询要淘汰的脏页个数太多，会导致查询的响应时间明显变长；
2. 日志写满，更新全部堵住，写性能跌为 0，这种情况对敏感业务来说，是不能接受的。



**3. MySQL 认为系统“空闲”的时候。**



**4.MySQL 正常关闭的情况。**



### 6. Mysql的双"1"配置



通常我们说 MySQL 的“双 1”配置，指的就是 sync_binlog 和 innodb_flush_log_at_trx_commit 都设置成 1。也就是说，一个事务完整提交前，需要等待两次刷盘，一次是 redo log（prepare 阶段），一次是 binlog。



这样可以保证提交的事务不会丢失。但是性能偏慢。



### 7. sync_binlog与innodb_flush_log_at_trx_commit的取值 背！



sync_binlog=0，表示MySQL不控制binlog的刷新，由文件系统自己控制它的缓存的刷新。这时候的性能是最好的，但是风险也是最大的。



sync_binlog=1，表示每次事务提交，MySQL都会把binlog刷下去，是最安全但是性能损耗最大的设置。



sync_binlog=n，当每进行n次事务提交之后，MySQL将进行一次fsync之类的磁盘同步指令来将binlog_cache中的数据强制写入磁盘。



innodb_flush_log_at_trx_commit的取值情况。



\1. 设置为 0 的时候，表示每次事务提交时都只是把 redo log 留在 redo log buffer 中 ;

\2. 设置为 1 的时候，表示每次事务提交时都将 redo log 直接持久化到磁盘；

\3. 设置为 2 的时候，表示每次事务提交时都只是把 redo log 写到 page cache。



InnoDB 有一个后台线程，每隔 1 秒，就会把 redo log buffer 中的日志，调用 write 写到文件系统的 page cache，然后调用 fsync 持久化到磁盘。





## 3. 索引



### 1. 常见的索引模型



1. 哈希表。哈希表这种结构适用于只有等值查询的场景，比如 Memcached 及其他一些 NoSQL 引擎。
2. 有序数组。有序数组索引只适用于静态存储引擎，更新数据成本太高了。
3. 搜索树。N 叉树由于在读写上的性能优点，以及适配磁盘的访问模式，已经被广泛应用在数据库引擎中了。



### 2. InnoDB 的索引模型



在 InnoDB 中，表都是根据主键顺序以索引的形式存放的，这种存储方式的表称为索引组织表。又因为前面我们提到的，InnoDB 使用了 B+ 树索引模型，所以数据都是存储在 B+ 树中的。**B+ 树能够很好地配合磁盘的读写特性，减少单次查询的磁盘访问次数。**每一个索引在 InnoDB 里面对应一棵 B+ 树。



### 3. 索引的优点和缺点 背！



**优点：**

1. 使⽤索引可以**⼤⼤加快数据的检索速度**（⼤⼤减少检索的数据ᰁ），这也是创建索引的最主要的原因。 
2. 但是注意使⽤索引不⼀定能够提⾼查询性能，因为如果数据库的数据不⼤，那么使⽤索引也不⼀定能够带来很大的提升。其余⼤多数情况下，索引查询⽐全表扫描要快。 
3. 通过创建唯⼀性索引，可以保证数据库表中每⼀行数据的唯⼀性。



**缺点：**

1. 空间消耗，⼀个索引对应的就是⼀棵 b+树，每⼀个节点都是⼀个 16KB ⼤⼩的⻚。**占⽤的空间较⼤**。 
2. 创建索引和维护索引需要耗费许多时间，当对表中的数据进⾏增删改的时候，如果数据有索引，那么索引也需要动态的修改，会降低 SQL 执⾏效率。



### 4. 主键索引和非主键索引的区别



主键索引的叶子节点存的是整行数据。在 InnoDB 里，主键索引也被称为聚簇索引（clustered index）。

非主键索引的叶子节点内容是主键的值。在 InnoDB 里，非主键索引也被称为二级索引（secondary index）。



通过非主键索引的查询还要多一次回表的过程。另外，主键长度越小，普通索引的叶子节点就越小，普通索引占用的空间也就越小。



### 5. 普通索引和唯一索引的区别



**定义：**

1. 普通索引是基本的索引类型，没有唯一性的限制，允许为 NULL 值。
2. 唯一索引数据列不允许重复，允许为 NULL 值，一个表允许多个列创建唯一索引。



**查询：**

1. 普通索引查到满⾜条件的第⼀个记录后，继续查找下⼀个记录，至到第⼀个不满⾜条件的记录。
2. 唯一索引由于索引唯⼀性，查到第⼀个满⾜条件的记录后，停⽌检索。
3. 但是，两者的性能差距微乎其微。因为InnoDB根据数据⻚来读写的。



**更新：**

1. 当数据页在内存中时，两者的执行过程基本一致，都是直接找到数据，进行更新。只不过唯一索引需要校验数据的唯一性。
2. 当数据页不在内存中时，由于唯一索引需要校验数据的唯一性，如果数据页不在内存中，需要先将数据页读入内存中，校验唯一性，然后再更新数据。而普通索引则是将更新记录在change buffer中，语句执行就结束了。



### 6. change buffer的理解 背！



将数据从磁盘读⼊内存涉及随机IO的访问，是数据库⾥⾯成本最⾼的操作之⼀。change buffer 因为减少了随机磁盘访问，所以对更新性能的提升很明显。



将 change buffer 中的操作应用到原数据页，得到最新结果的过程称为 merge。除了访问这个数据页会触发 merge 外，系统有后台线程会定期 merge。在数据库正常关闭（shutdown）的过程中，也会执行 merge 操作。



因为 merge 的时候是真正进行数据更新的时刻，而 change buffer 的主要目的就是将记录的变更动作缓存下来，所以在一个数据页做 merge 之前，change buffer 记录的变更越多（也就是这个页面上要更新的次数越多），收益就越大。



因此，对于写多读少的业务来说，页面在写完以后马上被访问到的概率比较小，此时 change buffer 的使用效果最好。这种业务模型常见的就是账单类、日志类的系统。



反过来，假设一个业务的更新模式是写入之后马上会做查询，那么即使满足了条件，将更新先记录在 change buffer，但之后由于马上要访问这个数据页，会立即触发 merge 过程。这样随机访问 IO 的次数不会减少，反而增加了 change buffer 的维护代价。所以，对于这种业务模式来说，change buffer 反而起到了副作用。



redo log 主要节省的是**随机写磁盘的 IO 消耗（转成顺序写）**，而 change buffer 主要节省的是**随机读磁盘的IO消耗**。



### 7. 什么是覆盖索引?



如果执行的语句是 select ID from T where k between 3 and 5，这时只需要查 ID 的值，而 ID 的值已经在 k 索引树上了，因此可以直接提供查询结果，不需要回表。也就是说，在这个查询里面，索引 k 已经“覆盖了”我们的查询需求，我们称为覆盖索引。

**由于覆盖索引可以减少树的搜索次数，显著提升查询性能，所以使用覆盖索引是一个常用的性能优化手段。**



### 8. 前缀索引与索引下推  



组合索引：在多个字段上创建的索引，只有在查询条件中使⽤了创建索引的第⼀个字段，索引才会被使⽤。



不只是索引的全部定义，只要满足最左前缀，就可以利用索引来加速检索。这个最左前缀可以是联合索引的最左 N 个字段，也可以是字符串索引的最左 M 个字符。



考虑到“最左前缀原则”，可以通过调整组合索引中的字段顺序，可以少维护⼀个索引。



- 🔲而 MySQL 5.6 引入的索引下推优化（index condition pushdown)， 可以在索引遍历过程中，对索引中包含的字段先做判断，直接过滤掉不满足条件的记录，减少回表次数。



### 9. 索引使用的注意事项 背！



1. 索引不是越多越好，虽然索引会提⾼ select 效率，但是也降低了insert以及update的效率
2. 数据⼩的表不需要建⽴索引，会增加额外的索引开销
3. 不经常使⽤的列不要建⽴索引
4. 频繁更新的列不要建⽴索引，会影响更新的效率
5. 尽避免在where字句中对字段进⾏空值判断，这会导致引擎放弃使⽤索引，进⾏全表扫描 
6. 字段值分布很稀少的字段，不适合建⽴索引
7. 不要⽤字符字段做主键
8. 字符字段只建⽴前缀索引
9. 不要⽤外键和UNIQUE 
10. 使⽤多列索引时，注意顺序和查询条件保持⼀致，同时删除不必要的单列索引

### 10. 索引失效的几种情况? 背！



1.模糊匹配当中以 “%” 开头时，索引失效。

2.OR 有⼀边的条件字段没有索引时，索引失效。 

3.使⽤复合索引的时候，没有使⽤左侧的列查找，索引失效。 

4.在 where 当中索引列参加了运算，索引失效 。

5.在where当中索引列使⽤了函数，索引失效。

6.索引列发生隐式类型转换时，索引失效。

7.索引列发生隐式字符编码转换时，索引失效。



## 4. 事务



### 1. MySQL事务特性



**A 原⼦性（Atomicity）** 

⼀个事务的所有操作，要么全部完成，要么都没完成，不能结束在中间环节。如果事务在执⾏过程中发⽣错误，会被回滚到事务开始之前的状态。



**C ⼀致性（Consistency）** 

在事务开始之前以及事务结束之后，数据库的完整性不能被破坏。



**I 隔离性（Isolation）** 

允许多个并发事务同时对数据进⾏修改和读写的能⼒，它可以防⽌由于多个事务并发执⾏时由于交叉执⾏⽽导致的数据不⼀致。



**D 持久性（Durability）** 

事务处理结束了以后，对数据的修改是永久的，即使是发⽣了系统故障，数据也不会丢失。



### 2.  Innodb的四种事务



**读未提交**（read uncommitted）



读未提交是指，一个事务还没提交时，它做的变更就能被别的事务看到。

在该隔离级别下，事务之间完全不隔离，会产⽣脏读，⼀般情况不会使⽤。



**读提交**（read committed）



读提交是指，一个事务提交之后，它做的变更才会被其他事务看到。

但有⼀个问题在同⼀个事务中，前后两相同的select可能会读到不同的结果。



**可重复读**（repeatable read）



可重复读是指，一个事务执行过程中看到的数据，总是跟这个事务在启动时看到的数据是一致的。当然在可重复读隔离级别下，未提交变更对其他事务也是不可见的。



**串行化**（serializable ）



串行化，顾名思义是对于同一行记录，“写”会加“写锁”，“读”会加“读锁”。当出现读写锁冲突的时候，后访问的事务必须等前一个事务执行完成，才能继续执行。



**Innodb默认的事务隔离级别是可重复性读，使用事务时应当避免长事务。**



### 3. Mysql的视图



一个是 view。它是一个用查询语句定义的虚拟表，在调用的时候执行查询语句并生成结果。创建视图的语法是 create view … ，而它的查询方法与表一样。



另一个是 InnoDB 在实现 MVCC 时用到的一致性读视图，即 consistent read view，用于支持 RC（Read Committed，读提交）和 RR（Repeatable Read，可重复读）隔离级别的实现。



### 4. 事务是如何实现mvcc的？ 背！



1. 每个事务都有⼀个事务ID,叫做transaction id(严格递增) 



1. 事务在启动时,找到已提交的最⼤事务ID记为up_limit_id。



1. 事务在更新⼀条语句时⽐如 id=1 改为了 id=2.会把 id=1 和该⾏之前的 row_trx_id 写到 undo log ⾥, 并且在数据⻚上把 id 的值改为 2,并且把修改这条语句的transaction id 记在该⾏⾏头。



1. ⼀个事务要查看⼀条数据时,必须先⽤该事务的 up_limit_id 与该⾏的transaction id 做⽐对。

- - 如果 up_limit_id >= transaction id,那么可以看 
  - 如果 up_limit_id < transaction id,则只能去 undo log ⾥去取。

去 undo log 查找数据的时候,也需要做⽐对,必须 up_limit_id > transaction id，才返回数据。



### 5. 什么是当前读？



更新数据都是先读后写的，而这个读，只能读当前的值，称为“当前读”（current read）。所以为当前读会更新事务内的 up_limit_id 为该事务的 transaction_id。



**这也就是为什么在RR隔离级别下依旧可以保证数据可顺利更新,并且更新过后读到的就是最新值。**



### 6. 什么是幻读？



幻读是指在同⼀个事务中，存在前后两次查询同⼀个范围的数据，但是第⼆次查询却看到了第⼀次查询没看到的⾏。



**幻读出现的场景：** 

\1. 事务的隔离级别为可重复读，且是当前读（**只是普通查询是不会看到别的事务插入的数据，也不会产生幻读**）

\2. 幻读仅专指新插⼊的⾏



**带来的问题：**

\1. 对⾏锁语义的破坏

\2. 破坏了数据⼀致性



**如何解决？**

存储引擎采⽤**加间隙锁**的⽅式来避免出现幻读。



**引入间隙锁会有什么问题？**

降低并发度，可能导致死锁。



## 5. 锁



### 1.  锁的种类



**锁一共有如下几种：全局锁、表锁、元数据锁、行锁、间隙锁。**



**全局锁**



顾名思义，全局锁就是**对整个数据库**实例加锁。MySQL 提供了一个加全局读锁的方法，命令是 Flush tables with read lock (FTWRL)。当你需要让整个库处于只读状态的时候，可以使用这个命令，之后其他线程的以下语句会被阻塞：数据更新语句（数据的增删改）、数据定义语句（包括建表、修改表结构等）和更新类事务的提交语句。



**表锁**



**表锁的语法是 lock tables … read/write。**与 FTWRL 类似，可以用 unlock tables 主动释放锁，也可以在客户端断开的时候自动释放。需要注意，lock tables 语法除了会限制别的线程的读写外，也限定了本线程接下来的操作对象。



**元数据锁（meta data lock，MDL）**



MDL 不需要显式使⽤，在访问⼀个表的时候会被⾃动加上。 



**MDL 的作⽤：保证读写的正确性。**



在 MySQL 5.5 版本中引⼊了 MDL。当对⼀个表做增删改查操作的时候，加 MDL 读锁，当要对表做结构变更操作的时候，加 MDL 写锁。 

读锁之间不互斥，因此你可以有多个线程同时对⼀张表增删改查。读写锁之间、写锁之间是互斥的，⽤来**保证变更表结构操作的安全性**。因此，如果有两个线程要同时给⼀个表加字段，其中⼀个要等另⼀个执⾏完才能开始执⾏。

事务中的 MDL 锁，在语句执⾏开始时申请，但是语句结束后并不会⻢上释放，⽽会等到整个事务提交后再释放。（这可能会产⽣死锁的问题）



**行锁**	



顾名思义，⾏锁就是针对数据表中记录的锁（也有⼈称为记录锁）。 

这很好理解，⽐如事务 A 更新了⼀⾏，⽽这时候事务 B 也要更新同⼀⾏，则必须等事务 A 的操作完成后才能进⾏更新。 



**特点：** 



\1. 每次操作锁住⼀⾏数据

\2. 开销⼤，加锁慢

\3. 发⽣锁冲突的概率是最低的，并发度是最⾼的



在 InnoDB 事务中，⾏锁是在需要的时候才加上的，但并不是不需要了就⽴刻释放，⽽是要等到事务结束时才释 

放。这个就是**两阶段锁协议**。



知道了这个设定，对我们使⽤事务有什么帮助呢？那就是，如果你的事务中需要锁多个⾏，要把最可能造成锁冲 

突、最可能影响并发度的锁尽往后放。



**间隙锁**



是专门⽤于解决幻读这种问题的锁，它锁的了⾏与⾏之间的间隙，**能够阻塞新插⼊的操作。**间隙锁在可重复读级别下才是有效的。



注意，读读不互斥，读写/写读/写写是互斥的，但是间隙锁之间是不冲突的，间隙锁会阻塞插⼊操作。



加锁的基本单位是 next-key lock，next-key lock 是前开后闭区间。MySQL 为了解决幻读问题，在线程更新数据并 next-key lock 的过程中，⾸先必须在可复读的隔离级别下，执⾏⾏锁和间隙锁合称 next-key lock，这个锁是左开右闭的区间。



### 2. 锁的划分



**共享锁**（共享锁也叫读锁或 S 锁 ）



共享锁锁定的资源可以被其他⽤户读取，但不能修改。 

在进⾏SELECT的时候，会将对象进⾏共享锁锁定，当数据读取完毕之后，就会释放共享锁，这样就可以保证数据 在读取时不被修改。 



**排他锁**（排它锁也叫独占锁、写锁或 X 锁）



排它锁锁定的数据只允许进⾏锁定操作的事务使⽤，其他事务⽆法对已锁定的数据进⾏查询或修改。

另外当我们对数据进⾏更新的时候，也就是INSERT、DELETE或者UPDATE的时候，数据库也会⾃动使⽤排它锁， 

防⽌其他事务对该数据⾏进⾏操作。



**乐观锁 （****Optimistic Locking****）**



认为**对同⼀数据的并发操作不会总发⽣**，属于小概率事件，**不⽤每次都对数据上锁**，也就是不采⽤数据库⾃身的锁机制，而是通过程序来实现。在程序上，我们可以采⽤版本号机制或者时间戳机制实现。



适合读操作多的场景，相对来说写的操作⽐较少。它的优点在于程序实现，不存在死锁问题，不过适⽤场景也会相对乐观，因为它阻⽌不了除了程序以外的数据库操作。



**悲观锁****（Pessimistic Locking）**



也是⼀种思想，对数据被其他事务的修改持**保守态度**，会通过数据库⾃身的锁机制来实现，从而保证数据操作的排它性。 



适合写操作多的场景，因为写的操作具有排它性。采⽤悲观锁的⽅式，可以在数据库层⾯阻⽌其他事务对该数据的操作权限，防⽌读 - 写和写 - 写的冲突。但是加锁的时间会⽐较⻓，可能会⻓时间限制其他⽤户的访问，也就是说他的并发访问性不好。



### 3. mysql意向锁的作用 背！



因为mysql存在行锁，而行锁与表锁存在互斥的关系，当执行一条语句出现行锁时，会修改表头的一个标志位，也就是所谓的意向锁。此时，如果需要锁表，会去判断标志位，从而迅速知道能否进行缩表。



## 6. 主从复制



### 1. 主从复制的原理

![img](https://cdn.nlark.com/yuque/0/2022/png/22228669/1642341010217-49eb0e23-4354-42bd-b9f2-ae5b517d9ff2.png?x-oss-process=image%2Fresize%2Cw_750%2Climit_0)

备库 B 跟主库 A 之间维持了一个长连接。主库 A 内部有一个线程，专门用于服务备库 B 的这个长连接。一个事务日志同步的完整过程是这样的：

1. 在备库 B 上通过 change master 命令，设置主库 A 的 IP、端口、用户名、密码，以及要从哪个位置开始请求 binlog，这个位置包含文件名和日志偏移量。
2. 在备库 B 上执行 start slave 命令，这时候备库会启动两个线程，就是图中的 io_thread 和 sql_thread。其中 io_thread 负责与主库建立连接。
3. 主库 A 校验完用户名、密码后，开始按照备库 B 传过来的位置，从本地读取 binlog，发给 B。
4. 备库 B 拿到 binlog 后，写到本地文件，称为中转日志（relay log）。
5. sql_thread 读取中转日志，解析出日志里的命令，并执行。



### 2. 主从复制的作用



**1、数据实时备份** 

当系统中某个节点发⽣故障时，可以⽅便故障切换。



**2****、读写分离** 

在开发过程中，如果遇到某个sql语句需要锁表，导致暂时不能使⽤读的服务。

使⽤主从复制，让主数据库负责写，从数据库负责读，即使主库出现锁表的情景，也可以通过从库正常读数据。



**3****、架构扩展** 

随着系统中业务访问ᰁ的增加，如果是单机部署数据，会导致I/O访问频率过⾼。通过主从复制，增加多个数据存储结点，将负载分布在多个从节点上，降低单机的I/O访问频率，提⾼单机的I/O性能。



### 3. 什么是主备延迟?



1. 主库 A 执行完成一个事务，写入 binlog，我们把这个时刻记为 T1;
2. 之后传给备库 B，我们把备库 B 接收完这个 binlog 的时刻记为 T2;
3. 备库 B 执行完成这个事务，我们把这个时刻记为 T3。

所谓主备延迟，就是同一个事务，在备库执行完成的时间和主库执行完成的时间之间的差值，也就是 T3-T1。



你可以在备库上执行 show slave status 命令，它的返回结果里面会显示 seconds_behind_master，用于表示当前备库延迟了多少秒。

主备延迟最直接的表现是，备库消费中转日志（relay log）的速度，比主库生产 binlog 的速度要慢。



### 4. 哪些因素会造成主备延迟？



1. 有些部署条件下，备库所在机器的性能要比主库所在的机器性能差。
2. 备库的压力大。
3. 执行了大事务。



### 5. 备库的并行复制

![img](https://cdn.nlark.com/yuque/0/2022/png/22228669/1645541100030-369514e5-2d6e-4587-811b-77250980b7d7.png)

图 2 中，coordinator 就是原来的 sql_thread, 不过现在它不再直接更新数据了，只负责读取中转日志和分发事务。真正更新日志的，变成了 worker 线程。而 work 线程的个数，就是由参数 slave_parallel_workers 决定的。根据我的经验，把这个值设置为 8~16 之间最好（32 核物理机的情况），毕竟备库还有可能要提供读查询，不能把 CPU 都吃光了。



**实现并行复制的两个基本要求：**

1. 不能造成更新覆盖。这就要求更新同一行的两个事务，必须被分发到同一个 worker 中。
2. 同一个事务不能被拆开，必须放到同一个 worker 中。



### 6. 备库的并行复制策略



在 2018 年 4 月份发布的 MySQL 5.7.22 版本里，MySQL 增加了一个新的并行复制策略，基于 WRITESET 的并行复制。



相应地，新增了一个参数 binlog-transaction-dependency-tracking，用来控制是否启用这个新策略。这个参数的可选值有以下三种：

1. COMMIT_ORDER，表示的就是前面介绍的，根据同时进入 prepare 和 commit 来判断是否可以并行的策略。
2. WRITESET，表示的是对于事务涉及更新的每一行，计算出这一行的 hash 值，组成集合 writeset。如果两个事务没有操作相同的行，也就是说它们的 writeset 没有交集，就可以并行。
3. WRITESET_SESSION，是在 WRITESET 的基础上多了一个约束，即在主库上同一个线程先后执行的两个事务，在备库执行的时候，要保证相同的先后顺序。



**WRITESET策略的优势：**

1. writeset 是在主库生成后直接写入到 binlog 里面的，这样在备库执行的时候，不需要解析 binlog 内容（event 里的行数据），节省了很多计算量；
2. 不需要把整个事务的 binlog 都扫一遍才能决定分发到哪个 worker，更省内存；
3. 由于备库的分发策略不依赖于 binlog 内容，所以 binlog 是 statement 格式也是可以的。



当然，对于“表上没主键”和“外键约束”的场景，WRITESET 策略也是没法并行的，也会暂时退化为单线程模型。

### 7. 切换主库的策略



1. **基于位点的主备切换**

```go
CHANGE MASTER TO 
MASTER_HOST=$host_name 
MASTER_PORT=$port 
MASTER_USER=$user_name 
MASTER_PASSWORD=$password 
MASTER_LOG_FILE=$master_log_name 
MASTER_LOG_POS=$master_log_pos
```

最后两个参数 MASTER_LOG_FILE 和 MASTER_LOG_POS 表示，要从主库的 master_log_name 文件的 master_log_pos 这个位置的日志继续同步。而这个位置就是我们所说的同步位点，也就是主库对应的文件名和日志偏移量。



**这种方法很难找到精确的同步位点，只能手动跳过错误。**



1. **GTID**



GTID 的全称是 Global Transaction Identifier，也就是全局事务 ID，是一个事务在提交的时候生成的，是这个事务的唯一标识。它由两部分组成，格式是：

```sql
GTID=server_uuid:gno
```

其中：

- server_uuid 是一个实例第一次启动时自动生成的，是一个全局唯一的值；
- gno 是一个整数，初始值是 1，每次提交事务的时候分配给这个事务，并加 1。



GTID 模式的启动也很简单，我们只需要在启动一个 MySQL 实例的时候，加上参数 gtid_mode=on 和 enforce_gtid_consistency=on 就可以了。在 GTID 模式下，每个事务都会跟一个 GTID 一一对应。



**每个mysql实例都会有一个 GTID 集合，用来对应“这个实例执行过的所有事务”。**



在 GTID 模式下，备库 B 要设置为新主库 A’的从库的语法如下：

```sql
CHANGE MASTER TO 
MASTER_HOST=$host_name 
MASTER_PORT=$port 
MASTER_USER=$user_name 
MASTER_PASSWORD=$password 
master_auto_position=1 
```

其中，master_auto_position=1 就表示这个主备关系使用的是 GTID 协议。



**切换主库后，会计算新主库与从库GTID集合的差集。从而让新的从库执行。但是要保证差集所对应的binlog在新主库中是完整的，不然就会报错。**





### 8. 如何处理过期读



**1.强制走主库方案。**



必须第一时间拿到结果的走主库，可以延迟的走从库。



**2.sleep 方案。**



主库更新后，读从库之前先 sleep 一下。具体的方案就是，类似于执行一条 select sleep(1) 命令。

这个方案的假设是，大多数情况下主备延迟在 1 秒之内，做一个 sleep 可以有很大概率拿到最新的数据。

**可以让前端先写死一个值，等客户刷新页面时再去数据库拿到最新的数据。**



**3.判断主备无延迟方案。**



主从无延迟方案，指的是在读从库之前，先判断主从是否有延迟，如果没有延迟，那么从从库读取，如果有延迟则等待一定的时间，若等待的时间内主从还是有延迟，可以返回报错或者去主库读取。但是如果超时去主库读取，可能会导致大量的请求打到主库上，为此主库需要做好限流。

### 

### 9. 如何判断主库挂了



**1.select 1判断**



select 1 成功返回，只能说明这个库的进程还在，并不能说明主库没问题。



**2.查表判断。**



为了能够检测 InnoDB 并发线程数过多导致的系统不可用情况，我们需要找一个访问 InnoDB 的场景。一般的做法是，在系统库（mysql 库）里创建一个表，比如命名为 health_check，里面只放一行数据，然后定期执行：

```sql
mysql> select * from mysql.health_check; 
```



使用这个方法，我们可以检测出由于并发线程过多导致的数据库不可用的情况。



但是，我们马上还会碰到下一个问题，即：空间满了以后，这种方法又会变得不好使。



我们知道，更新事务要写 binlog，而一旦 binlog 所在磁盘的空间占用率达到 100%，那么所有的更新语句和事务提交的 commit 语句就都会被堵住。但是，系统这时候还是可以正常读数据的。



**3.更新判断。**



既然要更新，就要放个有意义的字段，常见做法是放一个 timestamp 字段，用来表示最后一次执行检测的时间。这条更新语句类似于：

```sql
mysql> update mysql.health_check set t_modified=now();
```



节点可用性的检测都应该包含主库和备库。如果用更新来检测主库的话，那么备库也要进行更新检测。



**更新判断是一个相对比较常用的方案了，不过依然存在一些问题。其中，“判定慢”一直是让 DBA 头疼的问题。**



你可以设想一个日志盘的 IO 利用率已经是 100% 的场景。这时候，整个系统响应非常慢，已经需要做主备切换了。



但是你要知道，IO 利用率 100% 表示系统的 IO 是在工作的，每个请求都有机会获得 IO 资源，执行自己的任务。而我们的检测使用的 update 命令，需要的资源很少，所以可能在拿到 IO 资源的时候就可以提交成功，并且在超时时间 N 秒未到达之前就返回给了检测系统。



检测系统一看，update 命令没有超时，于是就得到了“系统正常”的结论。



也就是说，这时候在业务系统上正常的 SQL 语句已经执行得很慢了，但是 DBA 上去一看，HA 系统还在正常工作，并且认为主库现在处于可用状态。



**4.内部统计。**



MySQL 5.6 版本以后提供的 performance_schema 库，就在 file_summary_by_event_name 表里统计了每次 IO 请求的时间。



很简单，你可以通过 MAX_TIMER 的值来判断数据库是否出问题了。比如，你可以设定阈值，单次 IO 请求时间超过 200 毫秒属于异常，然后使用类似下面这条语句作为检测逻辑。

## 7. 分库分表



### 1. 分库分表的原因



**1、单库太大**

单个数据库处理能力有限，所在的服务器上的磁盘空间也有限，单库存在I/O操作瓶颈。

主要⽅案：切分成更多更小的库



**2、单表太大**

CRUD都成问题，索引膨胀，查询超时

主要⽅案：切分成多个数据集更小的表



### 2. 拆分方案 



**1、垂直拆分** 



垂直分表 

\1. “⼤表拆⼩表“，基于列的字段进⾏ 

\2. ⼀般表中字段较多，将不常⽤的，数据较大的，长度较⻓的，拆分到“扩展表” 



垂直分库

\1. ⼀般情况下，针对⼀个系统中不同业务进⾏拆分

\2. ⼀般我们拆分之后，放到多个服务器上



**2、水平拆分** 



水平分表

针对数据巨大的单张表（比如订单），按照某种规则（RANGE，HASH取模），切分到多张表中，这些表还在⼀个数据库中。



水平分库 

将单张表的数据切分到多个服务器上，每个服务器都有相应的库和表，只是表中的数据集合不同 

⽔平分库能够有效的缓解单机和单库的性能瓶颈，I/O，连接数和硬件资源等瓶颈



**水平分库分表切分规则：** 

**range：** 根据范围，⽐如0-1000⼀个表，1001到2000⼀个表

**hash****取模：** ⽐如取ID，进⾏hash取模，根据模数分配到不同的数据库中 

**地理区域：** 按照地理范围进⾏划分 

**时间范围：** 按照时间进⾏切分

### 3. 分库分表的中间件

- Cobar（阿里巴巴）
- MyCAT（基于Cobar）



### 4. 如何解决不同片的事务一致性的问题



一般可使用"XA协议"和"两阶段提交"处理。和分布式事务一致。



### 5. 如何解决跨库Join的问题 背！



**全局表**

全局表，也可看做是"数据字典表"，就是系统中所有模块都可能依赖的一些表，为了避免跨库join查询，可以将这类表在每个数据库中都保存一份。这些数据通常很少会进行修改，所以也不担心一致性的问题。



**字段冗余**

一种典型的反范式设计，利用空间换时间，为了性能而避免join查询。例如：订单表保存userId时候，也将userName冗余保存一份，这样查询订单详情时就不需要再去查询"买家user表"了。

但这种方法适用场景也有限，比较适用于依赖字段比较少的情况。而冗余字段的数据一致性也较难保证，就像上面订单表的例子，买家修改了userName后，是否需要在历史订单中同步更新呢？这也要结合实际业务场景进行考虑。



**数据组装**

在系统层面，分两次查询，第一次查询的结果集中找出关联数据id，然后根据id发起第二次请求得到关联数据。最后将获得到的数据进行字段拼装。



**ER分片**

关系型数据库中，如果可以先确定表之间的关联关系，并将那些存在关联关系的表记录存放在同一个分片上，那么就能较好的避免跨分片join问题。



### 6. 如何解决跨节点分页、排序、函数问题



跨节点多库进行查询时，会出现limit分页、order by排序等问题。分页需要按照指定字段进行排序，当排序字段就是分片字段时，通过分片规则就比较容易定位到指定的分片；当排序字段非分片字段时，就变得比较复杂了。需要先在不同的分片节点中将数据进行排序并返回，然后将不同分片返回的结果集进行汇总和再次排序，最终返回给用户。



### 7. 如何解决全局主键问题



**UUID**



UUID是主键是最简单的方案，本地生成，性能高，没有网络耗时。但缺点也很明显，由于UUID非常长，会占用大量的存储空间；另外，作为主键建立索引和基于索引进行查询时都会存在性能问题，在InnoDB下，UUID的无序性会引起数据位置频繁变动，导致分页。



**结合数据库维护主键ID表**



存在单点问题，强依赖DB，当DB异常时，整个系统都不可用。



**Snowflake分布式自增ID算法**



snowflake是Twitter开源的分布式ID生成算法，结果是一个long型的ID。其核心思想是：使用41bit作为毫秒数，10bit作为机器的ID（5个bit是数据中心，5个bit的机器ID），12bit作为毫秒内的流水号（意味着每个节点在每毫秒可以产生 4096 个 ID），最后还有一个符号位，永远是0。



不足就在于：强依赖机器时钟，如果时钟回拨，则可能导致生成ID重复。



**Leaf——美团点评分布式ID生成系统，并考虑到了高可用、容灾、分布式下时钟等问题。**



### 8. 如何解决数据迁移、扩容问题



当业务高速发展，面临性能和存储的瓶颈时，才会考虑分片设计，此时就不可避免的需要考虑历史数据迁移的问题。一般做法是先读出历史数据，然后按指定的分片规则再将数据写入到各个分片节点中。此外还需要根据当前的数据量和QPS，以及业务发展的速度，进行容量规划，推算出大概需要多少分片（一般建议单个分片上的单表数据量不超过1000W）



如果采用数值范围分片，只需要添加节点就可以进行扩容了，不需要对分片数据迁移。如果采用的是数值取模分片，则考虑后期的扩容问题就相对比较麻烦。