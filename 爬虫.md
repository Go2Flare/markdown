#  爬虫简介

编写程序，实现模拟浏览器发送请求，获取到和浏览器一摸一样的数据。因此，我们能获取到的是浏览器能接收到的数据

## 爬虫获取的数据的途径

- 呈现数据，呈现在app或网站上
- 进行数据分析，获得结论

## 爬虫的分类

- 通用爬虫：搜索引擎的爬虫
- 聚焦爬虫：针对特定网站的爬虫

**聚焦爬虫的工作流程**：

1. 明**确**URL(请求得知，明确爬什么)
2. **发**送请求，获取响应数据
3. 保存响应数据，提**取**有用信息
4. 处理数据（存储，使**用**）

**爬虫爬取哪些数据**：

资讯公司：特定领域的新闻数据的爬虫

金融公司：关于各个公司的动态的信息

酒店/旅游：协程，去哪儿的酒店价格信息/机票，景点价格，其他旅游公司价格信息

房地产、高铁：10大房地厂楼盘门户网站，政府动态等

强生保健医药：医疗数据，价格，目前的市场的行情

## 总结

爬虫概念：

- 访问web服务器，获取指定数据信息的程序

工作流程：

1. 明确目标URL
2. 发送请求，获取应答包
3. 包存过滤数据，提取有用信息
4. 使用，分析得到数据信息

**对爬虫整体做了简单了解后，我们实现一个爬虫案例，尝试爬取百度贴吧中的一些讯息**

# 爬取百度贴吧

## 分析目标URL规律：

分析url的param : pn 

得到每一页地址是前一页地址+50

```
https://tieba.baidu.com/f?kw=%E5%90%8E%E5%AE%AB%E5%8A%A8%E6%BC%AB%E5%90%A7%E5%8F%8B%E6%83%85%E8%B4%B4%E5%90%A7&ie=utf-8&pn=0

https://tieba.baidu.com/f?kw=%E5%90%8E%E5%AE%AB%E5%8A%A8%E6%BC%AB%E5%90%A7%E5%8F%8B%E6%83%85%E8%B4%B4%E5%90%A7&ie=utf-8&pn=50

https://tieba.baidu.com/f?kw=%E5%90%8E%E5%AE%AB%E5%8A%A8%E6%BC%AB%E5%90%A7%E5%8F%8B%E6%83%85%E8%B4%B4%E5%90%A7&ie=utf-8&pn=100

https://tieba.baidu.com/f?kw=%E5%90%8E%E5%AE%AB%E5%8A%A8%E6%BC%AB%E5%90%A7%E5%8F%8B%E6%83%85%E8%B4%B4%E5%90%A7&ie=utf-8&pn=150
```

## 省略

### 发送请求，获取响应（将所有的网站内容全部爬下来）

### 提取数据，去掉对我们没用处的数据

### 处理数据（按照我们想要的方式存储和使用）

## 代码

- 基本实现:

```go
func main(){
	var start,end int
	fmt.Print("请输入爬取的起始页（>=1）: ")
	fmt.Scanln(&start)
	fmt.Print("请输入结束的起始页（>=start）: ")
	fmt.Scanln(&end)
	crawler(start, end)
}

//爬取函数
func crawler(start int, end int) {
	fmt.Printf("正在爬取第%v页-第%v页\n",start,end)
	//循环爬取每一页数据
	var url string
	for i:=start-1; i<end; i++{
		url = "https://tieba.baidu.com/f?kw=%E5%90%8E%E5%AE%AB%E5%8A%A8%E6%BC%AB%E5%90%A7%E5%8F%8B%E6%83%85%E8%B4%B4%E5%90%A7&ie=utf-8&pn="
		url += strconv.Itoa(start*50)//页码规律
		res, err := HttpGet(url)
		if err != nil{
			log.Fatalf("Call HttpGet err : %v", err)
		}
		//fmt.Println("res = ", res)

		//将读到的网页数据，保存至文件（源码的相对路径下）
		f, err := os.Create("第"+strconv.Itoa(i+1)+"页"+".html")
		if err != nil{
			log.Fatalf("os.Create : %v", err)
		}
		//保存一个文件，关闭一个文件，不用defer，因为defer是延迟到函数关闭才关闭
		//我们需要循环结束就关闭一次
		f.WriteString(res)
		f.Close()
	}
	log.Println("爬取完毕~")
}

func HttpGet(url string) (res string, err error){
	rsp, err := http.Get(url)
	if err != nil{
		log.Fatal("http.Get err : %v", err)
	}
	defer rsp.Body.Close()

//	循环读取一个网页数据，传出
	buf := make([]byte, 4096)
	for {
		n, err := rsp.Body.Read(buf)
		if n==0{break}
		if err!=nil && err!=io.EOF{
			log.Fatalf("rsp.Body.Read err : %v", err)
		}
		//每次循环读取到的切片数据合并到res
		res += string(buf[:n])
	}
	return
}
```

- 并发版:

1. sync.WaitGroup

```go

var wg sync.WaitGroup//不用指针
//爬取函数
func crawler(start int, end int) {
	wg.Add(end-start+1)
	defer wg.Wait() //记得关闭
	fmt.Printf("正在爬取第%v页-第%v页\n",start,end)
	//循环爬取每一页数据
	for i:=start-1; i<end; i++{
		//做并发，需要这些go程之间做同步
		go crawlerOnePage(i)
	}
	defer log.Println("爬取完毕~")
}

func crawlerOnePage(i int){
	defer wg.Done() //记得关闭
	url := "https://tieba.baidu.com/f?kw=%E5%90%8E%E5%AE%AB%E5%8A%A8%E6%BC%AB%E5%90%A7%E5%8F%8B%E6%83%85%E8%B4%B4%E5%90%A7&ie=utf-8&pn="
	url += strconv.Itoa(i*50)//页码规律
	res, err := HttpGet(url)
	if err != nil{
		log.Fatalf("Call HttpGet err : %v", err)
	}
	//fmt.Println("res = ", res)

	//将读到的网页数据，保存至文件（源码的相对路径下）
	f, err := os.Create("第"+strconv.Itoa(i+1)+"页"+".html")
	if err != nil{
		log.Fatalf("os.Create : %v", err)
	}
	//保存一个文件，关闭一个文件，不用defer，因为defer是延迟到函数关闭才关闭
	//我们需要循环结束就关闭一次
	f.WriteString(res)
	f.Close()
}
```

2. channel

```go

//爬取函数
func crawler(start int, end int) {

	fmt.Printf("正在爬取第%v页-第%v页\n",start,end)

	//协程信息的管道
	pages := make(chan int)

	//循环爬取每一页数据
	for i:=start-1; i<end; i++{
		//做并发，需要这些go程之间同步
		go crawlerOnePage(i, pages)
	}

	//全部取出管道接收的数据即可，注意，需要保证管道读写的平衡
	for i:=start-1; i<end; i++{
		log.Printf("第%v个页面爬取完成\n", <-pages)
	}
	defer log.Println("爬取完毕~")
}
//爬取一页
func crawlerOnePage(i int, pages chan int){
	url := "https://tieba.baidu.com/f?kw=%E5%90%8E%E5%AE%AB%E5%8A%A8%E6%BC%AB%E5%90%A7%E5%8F%8B%E6%83%85%E8%B4%B4%E5%90%A7&ie=utf-8&pn="
	url += strconv.Itoa(i*50)//页码规律
	res, err := HttpGet(url)
	if err != nil{
		log.Fatalf("Call HttpGet err : %v", err)
	}

	//将读到的网页数据，保存至文件（源码的相对路径下）
	f, err := os.Create("第"+strconv.Itoa(i+1)+"页"+".html")
	if err != nil{
		log.Fatalf("os.Create : %v", err)
	}
	//保存一个文件，关闭一个文件，不用defer，因为defer是延迟到函数关闭才关闭
	//我们需要循环结束就关闭一次
	f.WriteString(res)
	f.Close()

    //写入channel
	pages <- i
}
```



## 总结

简单实现：

- 指定起始，终止页，创建working函数

- 使用start，end表示循环条件爬取每页数据

- 获取每一页的URL：下一页=前一页+50

- 封装，实现HttpGet()函数，爬取每一个网页的数据内容，通过resule返回

  Http.Get / resp.Body.Close /buf := make(4096) /for (rsp.Body.Read(buf)) / res += string(buf[:n])

- 调用后使用循环因子i规律输出html文件

并发版：

- 封装 爬取一个页面内容的代码到crawlerOnePage(i int)函数中

- 在working函数for循环启动go程带调用crawlerOnePage -> n个待爬取页面

- 为防止主go程提前结束，应入channel实现同步，crawlerOnePage(i int, pages chan int)

- 在crawlerOnePage结尾处加上写入channel的逻辑

- 在循环写入channel的逻辑外，添加循环读取channel的逻辑，

  注意：**读写最好平衡**

# 正则表达式

GO语言通过regexp（regular expression）标准包为正则表达式提供了官方支持

## 基本语法

### 元字符

构造正则表达式的基本元素

| 元字符 | 说明                         |
| :----- | :--------------------------- |
| .      | 匹配除换行符以外的任意字符   |
| \w     | 匹配字母或数字或下划线或汉字 |
| \s     | 匹配任意的空白符             |
| \d     | 匹配数字                     |
| \b     | 匹配单词的开始或结束         |
| ^      | 匹配字符串的开始             |
| $      | 匹配字符串的结束             |

### 重复限定符

用来限定符表示字符串中“重复”的部分

| 语法  | 说明                 |
| :---- | :------------------- |
| *     | 重复零次或更多次     |
| +     | **重复一次或更多次** |
| ?     | 重复零次或一次       |
| {n}   | 重复n次              |
| {n,}  | 重复n次或更多次      |
| {n,m} | 重复n到m次           |

- 元字符-重复限定符简单实例：

1. 匹配有abc开头的字符串：

   ```shell
   \babc
   ^abc
   ```

2. 匹配8位数字的QQ号码：

   ```
   ^\d\d\d\d\d\d\d\d$
   ^\d{8}$
   ```

3. 匹配1开头11位数字的手机号码：

   ```
   ^1\d\d\d\d\d\d\d\d\d\d$
   ^1[3-9]\d{9}$
   ```

4. 匹配身份证号

   ```shell
   ^\d{17}[0-9Xx]|\d{15}$
   ```

5. 匹配银行卡号是14~18位的数字

   ```
   ^\d{14,18}$
   ```

6. 匹配以a开头的，0个或多个b结尾的字符串

   ```
   ^ab*$
   ```

### 分组

​		重复限定符作用于左边最近的字符，如果需要多个字符呢？

> 我们需要小括号来分组，括号中的为一个整体

​		因此当我们需要匹配多个ab时

```
^(ab)*$
```

​		()会把每个分组里的匹配的值保存起来，使用$n(n是数字，可捕获第n个捕获组的内容)

​		(?:)表示非捕获组，和捕获分组唯一的区别在于，非捕获分组匹配的值不会保存起来

```
// 数字格式化 1,123,000
"1234567890".replace(/\B(?=(?:\d{3})+(?!\d))/g,",") // 结果：1,234,567,890，匹配的是后面是3*n个数字的非单词边界(\B)
```

### 转义

​		我们看到正则表达式用小括号来做分组，那转义呢？

> 使用通用的斜杠 \

​		匹配以 (ab) 开头

```
^(\(ab\))$
```

### 条件或

> 使用  |

​		匹配国内运营商电话号码

```
^(130|131|132|155|156|185|186|145|176)\d{8}$
```

### 区间

> 中括号表示区间条件
>
> 1. 限定0到9 可以写成[0-9]
> 2. 限定A-Z 写成[A-Z]
> 3. 限定某些数字 [165]
> 4. 区间中^为取反，表示排除^后面的元素

​		所以上面一个还可简化成

```
^((13[0-2])|(15[56])|(18[5-6])|145|176)\d{8}$
```

### 前瞻，后顾，负前瞻，负后顾

```
// 前瞻：
"windows2000" 
exp1(?=exp2) 查找exp2前面的exp1
windows(?=95|98|NT|2000)  "windows"
// 负前瞻：
"windows3.1"
exp1(?!exp2) 查找后面不是exp2的exp1
windows(?!95|98|NT|2000) -> "windows"
// 后顾：
"2000windows"
(?<=exp2)exp1 查找exp2后面的exp1
(?<=95|98|NT|2000)windows -> "windows"
// 负后顾：
"3.1windows"
(?<!exp2)exp1 查找前面不是exp2的exp1
(?<!95|98|NT|2000)windows -> "windows"
```



### 总结

能使用strings，strconv等包函数解决的问题，首选库函数，其次再选择正则表达式

# GO使用正则

通常来说，Go中使用正则表达式只需要两步即可

1. **解析，编译正则表达式**：使用regexp.MustCompile()函数

   ```go
   func MustCompile(str string) *Regexp
   ```

   函数的主要作用是将正则表达式中，奇怪的符号（如.*?\[...）转换成Go语言能识别的格式，并将其存成结构体格式，方便编译器识别

   **参数**：正则表达式子串，建议使用反引号

   **返回值**：编译后的结构体。解析失败是会产生panic

2. 根据解析好的规则（结构体形式）， 从指定字符串中提取需要的信息。使用FindAllStringSubmatch()函数

   ```go
   func (re *Regexp) FindAllStringSubmatch(s string, n int) [][]string
   ```

   **参数1**：待解析的字符串

   **参数2**：匹配的次数。通常传-1，表示匹配所有

   **返回值**：返回成功匹配的\[\]\[\]string

## 实例

demo1：

```go
func main(){
	//MatchString返回的第一个参数是bool类型即匹配结果，第二个参数是error类型
	sourceStr := `my email is gerrylon@163.com`
	//[匹配任意字符-]@任意字符(:.[任意字符])

	ret := regexp.MustCompile(`[\w-]+@[\w]+(?:\.[\w]+)`)//匹配邮箱
	alls := ret.FindAllStringSubmatch(sourceStr, -1)
	matched, _ := regexp.MatchString(`[\w-]+@[\w]+(?:\.[\w]+)+`, sourceStr)
	fmt.Printf("matchStr = %v, isMathch = %v\n", alls, matched) // true

	sourceStr = `
test text     lljflsdfjdskal
gerrylon@163.com
abc@gmail.com
someone@sina.com.cn`

	//@后匹配多个地址
	re := regexp.MustCompile(`[\w-]+@([\w]+(?:\.[\w]+)+)`)
	//-1默认全局匹配，其他正整数表示匹配的个数
	matchSlice := re.FindAllStringSubmatch(sourceStr, -1)
	fmt.Println(matchSlice)
	for _, match := range matchSlice {
		fmt.Printf("email is: %s, domain is: %s\n", match[0], match[1])
	}
	//email is: gerrylon@163.com, domain is: 163.com
	//email is: abc@gmail.com, domain is: gmail.com
	//email is: someone@sina.com.cn, domain is: sina.com.cn

	sourceStr = "peach"
	//r := regexp.MustCompile(`p([a-z]+)ch`)//MustCompile相较不返回错误
	r,_:=regexp.Compile(`p([a-z]+)ch`)//匹配peach
	alls = r.FindAllStringSubmatch(sourceStr, 1)
	matched = r.MatchString(sourceStr)
	fmt.Printf("matchStr = %v, isMathch = %v\n", alls, matched) // true

	//查找匹配的字符串
	fmt.Println(r.FindString("peach punch"))  //打印结果：peach

	//查找匹配字符串开始和结束位置的索引，而不是匹配内容[0 5]
	fmt.Println(r.FindStringIndex("peach punch"))  //打印结果： [0 5]

	//返回完全匹配和局部匹配的字符串，例如，这里会返回  p([a-z]+)ch 和 `([a-z]+) 的信息
	fmt.Println(r.FindStringSubmatch("peach punch"))   //打印结果：[peach ea]

	//返回完全匹配和局部匹配的索引位置
	fmt.Println(r.FindStringSubmatchIndex("peach punch"))   //打印结果： [0 5 1 3]

	//返回所有的匹配项，而不仅仅是首次匹配项。正整数用来限制匹配次数
	fmt.Println(r.FindAllString("peach punch pinch",-1))  //打印结果：[peach punch pinch]
	fmt.Println(r.FindAllString("peach punch pinch",2)) //匹配两次   打印结果：[peach punch]

	//返回所有的完全匹配和局部匹配的索引位置
	fmt.Println(r.FindAllStringSubmatchIndex("peach punch pinch",-1))
	//打印结果： [[0 5 1 3] [6 11 7 9] [12 17 13 15]]

	//上面的例子中，我们使用了字符串作为参数，并使用了如 MatchString 这样的方法。
	//我们也可以提供 []byte参数并将 String 从函数命中去掉。
	fmt.Println(r.Match([]byte("peach")))    //打印结果：true

	//替换
	//将匹配的结果，替换成新输入的结果
	fmt.Println(r.ReplaceAllString("a peach","<fruit>"))     //打印结果： a <fruit>

	//Func 变量允许传递匹配内容到一个给定的函数中，
	in:=[]byte("a peach")
	out:=r.ReplaceAllFunc(in,bytes.ToUpper)
	fmt.Printf(string(out)) //打印结果：   a PEACH
}
```

## 标签数据

测试发现,新添加的<div></div>标签中，如果出现换行，则源正则表达式‘<div>(.\*)<div>'不能正确提取数据。

因此需要调整正则表达式: '<div>(?s:(.\*?))</div>'。

分析这个表达式，有两部分内容。

(?s)是正则表达式的模式修饰符。即 Singleline(单行模式)。表示更改.的含义。使它与每一个字符匹配（包括换行符\n)。
(.\*?)是一个单元分组。".”匹配任意字符。“*?”表重复>=0次匹配。
这个语法，在正则表达式知识里是较难的应用，不必过度学习。

**结论**：将(?s:(.*?))元组放置于某一特征字串中，可以提取带有这一特征字串的内容。

实例：

```go
func main(){
	sourceStr := `
<!DOCTYPE html>
<html lang="zh-CN">
<head>
	<title>Go语言标准库文档中文版 | Go语言中文网 | Golang中文社区 | Golang中国</title>
	<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1.0, user-scalable=no">
	<meta http-equiv="X-UA-Compatible" content="IE=edge, chrome=1">
	<meta charset="utf-8">
	<link rel="shortcut icon" href="/static/img/go.ico">
	<link rel="apple-touch-icon" type="image/png" href="/static/img/logo2.png">
	<meta name="author" content="polaris <polaris@studygolang.com>">
	<meta name="keywords" content="中文, 文档, 标准库, Go语言,Golang,Go社区,Go中文社区,Golang中文社区,Go语言社区,Go语言学习,学习Go语言,Go语言学习园地,Golang 中国,Golang中国,Golang China, Go语言论坛, Go语言中文网">
	<meta name="description" content="Go语言文档中文版，Go语言中文网，中国 Golang 社区，Go语言学习园地，致力于构建完善的 Golang 中文社区，Go语言爱好者的学习家园。分享 Go 语言知识，交流使用经验">
	<title></title>
	<dev>hello 1</dev>
	<dev>hello 222</dev>
	<dev>hello 对对对对</dev>
	<dev>hello 喜喜</dev>
	<dev>
		唠几块钱的？
	</dev>
	<body>圣体</body>
</head>
<frameset cols="15,85">
	<frame src="/static/pkgdoc/i.html">
	<frame name="main" src="/static/pkgdoc/main.html" tppabs="main.html" >
	<noframes>
	</noframes>
</frameset>
</html>`
	//正则
	//regStr := `<dev>(.*)</dev>`//不能匹配多行
	regStr := `<dev>(?s:(.*?))</dev>`//?s:单行模式，能匹配换行

	r := regexp.MustCompile(regStr)
	allSlice := r.FindAllStringSubmatch(sourceStr, -1)
	for _, slice := range allSlice{
		fmt.Printf("%v\n",slice)
	}
}
```

# 爬取豆瓣电影

## 双向爬取

- 横向：以页为单位
- 纵向：以一个页面内的条目为单位

### 横向

```
https://movie.douban.com/top250?start=0&filter=

https://movie.douban.com/top250?start=25&filter=

https://movie.douban.com/top250?start=50&filter=
```

### 纵向

- 取电影名

```
<img width="100" alt="电影名"
#`<img width="100" alt="(?s:(.*?))"`
```

- 取分数

```
<span class="rating_num" property="v:average">分数</span>
#`<span class="rating_num" property="v:average">(?s:(.*?))</span>`
```

- 评价人数

```
<span>1627122人评价</span>
#`<span>(?s:(.*?))人评价</span>`
```

### 并发代码

main

```go
func main(){
	var start,end int
	fmt.Print("请输入爬取的起始页（>=1）: ")
	fmt.Scanln(&start)
	fmt.Print("请输入结束的起始页（>=start）: ")
	fmt.Scanln(&end)
	//使用channel进行go程同步
	s := time.Now()
	UseChannelCrawler(start, end)
	//使用waitGroup进行go程同步
	//UseWGCrawler(start, end)
	fmt.Println("花费时间",time.Since(s))
}
```

useChannel

```go
func UseChannelCrawler(start int, end int) {
	fmt.Printf("正在爬取%d到%d页\n", start, end)
	pages := make(chan int)
	for i:=start-1; i<end; i++{
		go UseChannelToGetPage(i, pages)
	}
	for i:=start-1; i<end; i++{
		fmt.Printf("第%v个页面爬取完毕\n", <- pages)
	}
	fmt.Println("爬取完毕")
}

func UseChannelToGetPage(i int, pages chan int){
	GetOnePage(i)
	pages<-i
}
```

useWaitGroup

```go
var wg sync.WaitGroup

func UseWGCrawler(start int, end int) {
	fmt.Printf("正在爬取%d到%d页\n", start, end)
	wg.Add(end-start+1)
	defer wg.Wait()
	for i:=start-1; i<end; i++{
		go UseWGToGetPage(i)
	}
	defer fmt.Println("爬取完毕")
}

func UseWGToGetPage(i int) {
	defer wg.Done()
	GetOnePage(i)
}
```

utils

```go

//爬取一个页面
func GetOnePage(i int) {
	//封装HttpGet爬取url对应页面
	url := fmt.Sprintf("https://movie.douban.com/top250?start=%s&filter=",strconv.Itoa(i*25))
	res, err := HttpGet(url)
	if err != nil{
		log.Fatalf("Call HttpGet err : %v", err)
	}
	// 解析编译正则
	r := regexp.MustCompile(`<img width="100" alt="(?s:(.*?))"`)
	firmNames := r.FindAllStringSubmatch(res, -1)

	r = regexp.MustCompile(`<span class="rating_num" property="v:average">(?s:(.*?))</span>`)
	rates := r.FindAllStringSubmatch(res, -1)

	r = regexp.MustCompile(`<span>(?s:(\d*?))人评价</span>`)
	evaluatePeopleNums := r.FindAllStringSubmatch(res, -1)

	Save2File(i, firmNames, rates, evaluatePeopleNums)
}

//封装http请求
func HttpGet(url string) (res string, err error){
	//rsp, err := http.Get(url)//豆瓣爬数据需要请求需要header里的user-agent，需要配置以下
	client := http.Client{}
	req, err := http.NewRequest("GET", url, nil)
	if err != nil{
		log.Fatalf("http.NewRequest err : %v", err)
	}
	req.Header.Set("user-agent", "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/98.0.4758.102 Safari/537.36")
	rsp, err := client.Do(req)//设置好header再发送
	if err != nil{
		log.Fatalf("http.Get err : %v", err)
	}
	defer rsp.Body.Close()

	//	循环用切片接收body数据
	buf := make([]byte, 4096)
	//	读完合并res
	bs := strings.Builder{}
	for{
		//n是当前接收的长度
		n, err := rsp.Body.Read(buf)
		if n==0 {break}
		if err != nil && err != io.EOF{
			log.Fatalf("rsp.Body.Read err : %v", err)
		}
		bs.Write(buf[:n])
	}
	return bs.String(),nil
}


func Save2File(i int, firmNames, rates, evaluatePeopleNums [][]string) {
	f, err := os.Create("第"+strconv.Itoa(i+1)+"页.txt")
	defer f.Close()
	if err != nil{
		log.Fatalf("os.Create : %v", err)
	}
	for j:=0; j<len(firmNames); j++{
		f.WriteString(fmt.Sprintf("name: %v, rate: %v, evaluatePeopleNum: %v\n", firmNames[j][1],
			rates[j][1], evaluatePeopleNums[j][1]))
	}
}
```

# 爬取段子

## 需求

纵向：需要一个页面中，取出所有段子和其url，再进入url获取详细段子信息

## 爬取

横向：

```
https://duanzixing.com/page/1/
https://duanzixing.com/page/2/
https://duanzixing.com/page/3/
```

纵向：

```
//段子url特征--12处
<a class="pc" href="https://duanzixing.com/1221.html#respond">
#`<a class="pc" href="(?s:(.*?))">`
//进入网页获取标题和内容
<title>同事今天没吃早餐 -  段子网-最新段子-搞笑段子-微段子-段子网-段子手-段子大全</title>
#<title>(?s:(.*?)) -  段子网-最新段子-搞笑段子-微段子-段子网-段子手-段子大全</title>
<article class="article-content">
                    <p>情人节一大早女神就给我发了节日的问候。</p><p>我跟朋友说：女神给我发消息了。</p><p>朋友说：就你这样的会有人看上你？</p><p>我说：我怎么了，女神真的给我发消息了。</p><p>朋友说：你的女神该不会是发了你手机欠费吧？</p><p>我说：你怎么知道的？</p><p>朋友：你女神真的是10086啊。</p>                </article>
#<article class="article-content">(?s:(.*?))</article>
```

## 并发代码

```go

func main(){
	var start,end int
	fmt.Print("请输入爬取的起始页（>=1）: ")
	fmt.Scanln(&start)
	fmt.Print("请输入结束的起始页（>=start）: ")
	fmt.Scanln(&end)
	//使用channel进行go程同步
	s := time.Now()
	UseChannelCrawler(start, end)
	//使用waitGroup进行go程同步
	//UseWGCrawler(start, end)
	fmt.Println("花费时间",time.Since(s))
}
//用channel同步go程
func UseChannelCrawler(start int, end int) {
	fmt.Printf("正在爬取%d到%d页\n", start, end)
	pages := make(chan int)
	for i:=start; i<=end; i++{
		go ChannelGetOnePage(i, pages)
	}
	for i:=start; i<=end; i++{
		fmt.Printf("已爬取第%v个页面\n",<-pages)
	}
}

func ChannelGetOnePage(i int, pages chan int){
	GetOnePage(i)
	pages<-i
}
//爬取一个页面的url
func GetOnePage(i int){
	url := fmt.Sprintf("https://duanzixing.com/page/%v/", i)
	res, err := HttpGet(url)
	if err != nil{
		log.Fatalf("Call HttpGet : %v", err)
	}
	r := regexp.MustCompile(`<a class="pc" href="(?s:(.*?))">`)
	urls := r.FindAllStringSubmatch(res, -1)
	realTitles, realContext := []string{}, []string{}
	for _, jokeUrl := range urls{
		title,context := GetOneJokePage(jokeUrl[1])
		realTitles = append(realTitles, title)
		realContext = append(realContext, context)
	}
	Save2File(i, realTitles, realContext)
}
//通过url获取详细的段子信息
func GetOneJokePage(url string) (string, string){
	res, err := HttpGet(url)
	if err != nil{
		log.Fatalf("Call HttpGet : %v", err)
	}
	r := regexp.MustCompile(`<title>(?s:(.*?)) -  段子网-最新段子-搞笑段子-微段子-段子网-段子手-段子大全</title>`)
	titles := r.FindAllStringSubmatch(res, -1)
	r = regexp.MustCompile(`<article class="article-content">(?s:(.*?))</article>`)
	contents := r.FindAllStringSubmatch(res, -1)
	for i:=0; i<len(contents); i++{
		contents[i][1] = strProcess(contents[i][1])
	}
	return titles[0][1], contents[0][1]
}
//保存文件
func Save2File(i int,titles []string, contents []string) {
	f,err := os.Create("段子第"+strconv.Itoa(i)+"页.html")
	defer f.Close()
	if err != nil{
		log.Fatalf("os.Create : %v", err)
	}
	for j:=0; j<len(titles); j++{
		f.WriteString("标题:"+titles[j]+"\n")
		f.WriteString("段子:\n"+contents[j]+"\n\n")
	}
}

//字符串处理
func strProcess(s string) string {
	b := []rune(s)
	i,j:=0,len(b)-1
	for ;i<len(b)&&(b[i] > 40869 || b[i] < 19968);i++{}
	for ;j>=0&&(b[j] >40869 || b[j] < 19968);j--{}
	if i<=j{
		b = b[i:j+1]
	}
	s = string(b)
	s = strings.ReplaceAll(s, "</p><p>", "\n")
	s = strings.ReplaceAll(s, "<br>", "\n")
	return s
}

//http请求
func HttpGet(url string) (res string, err error) {
	rsp, err := http.Get(url)
	if err != nil{
		log.Fatalf("http.NewRequest err : %v", err)
	}
	defer rsp.Body.Close()

	buf := make([]byte, 4096)
	for {
		n, err := rsp.Body.Read(buf)
		if n==0{break}
		if err != nil && err != io.EOF{
			log.Fatalf("rsp.Body.Read err : %v", err)
		}
		res += string(buf[:n])
	}
	return
}
```

# 爬取斗鱼图片，动图

## 需求

纵向：爬取图片

```
//封面
{"ratio":2.0,"rs16" :"(?s:(.*?))","type":"image/webp"}]

```

headers

```
:authority: hf1.hitomi.la
:method: GET
:path: /pc/2153603
:scheme: https
accept: */*
accept-encoding: gzip, deflate, br
accept-language: zh-CN,zh;q=0.9
origin: https://hitomi.la
referer: https://hitomi.la/reader/2153603.html
sec-ch-ua: " Not A;Brand";v="99", "Chromium";v="98", "Google Chrome";v="98"
sec-ch-ua-mobile: ?0
sec-ch-ua-platform: "Windows"
sec-fetch-dest: empty
sec-fetch-mode: cors
sec-fetch-site: same-site
user-agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/98.0.4758.102 Safari/537.36
```

